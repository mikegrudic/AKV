// This file was automatically generated by 'make' from file 'hyb_spat_to_SH.gen.c'.
// To modify it, please consider modifying hyb_spat_to_SH.gen.c
/*
 * Copyright (c) 2010-2013 Centre National de la Recherche Scientifique.
 * written by Nathanael Schaeffer (CNRS, ISTerre, Grenoble, France).
 * 
 * nathanael.schaeffer@ujf-grenoble.fr
 * 
 * This software is governed by the CeCILL license under French law and
 * abiding by the rules of distribution of free software. You can use,
 * modify and/or redistribute the software under the terms of the CeCILL
 * license as circulated by CEA, CNRS and INRIA at the following URL
 * "http://www.cecill.info".
 * 
 * The fact that you are presently reading this means that you have had
 * knowledge of the CeCILL license and that you accept its terms.
 * 
 */

//////////////////////////////////////////////////
  #ifdef SHT_AXISYM
/// The spatial field is assumed to be \b axisymmetric (spatial size NLAT), and only the m=0 harmonics are written to output.
  #endif

/// Truncation and spatial discretization are defined by \ref shtns_create and \ref shtns_set_grid_*
/// \param[in] shtns = a configuration created by \ref shtns_create with a grid set by shtns_set_grid_*
/// \param[in] Vr = spatial scalar field : double array.
/// \param[out] Qlm = spherical harmonics coefficients :
/// complex double arrays of size shtns->nlm.
  #ifdef SHT_VAR_LTR
/// \param[in] llim = specify maximum degree of spherical harmonic. llim must be at most shtns->lmax, and all spherical harmonic degree higher than llim are set to zero. 
  #endif

	static void GEN3(spat_to_SH_,ID_NME,SUFFIX)(shtns_cfg shtns, double *Vr, complex double *Qlm, long int llim) {
  #ifndef SHT_GRAD
  #else
  #endif

	double *zl;
	long int i,i0, ni,l;
  #ifndef SHT_AXISYM
	unsigned im, imlim;
	complex double *BrF;		// contains the Fourier transformed data
	v2d reo[2*NLAT_2];	// symmetric (even) and anti-symmetric (odd) parts, interleaved.
	#define reo0 ((double*)reo)
  #else
	double reo0[2*NLAT_2+2] SSE;	// symmetric (even) and anti-symmetric (odd) parts, interleaved.
  #endif

	#define ZL(i) vdup(zl[i])

// defines how to access even and odd parts of data
	#define re(i)	reo[2*(i)]
	#define ro(i)	reo[2*(i)+1]
	#define re0(i)	reo0[2*(i)+1]
	#define ro0(i)	reo0[2*(i)]

  #ifndef SHT_AXISYM
	BrF = (complex double *) Vr;
	if (shtns->ncplx_fft >= 0) {
	    if (shtns->ncplx_fft > 0) {		// alloc memory for the FFT
	    	BrF = VMALLOC( shtns->ncplx_fft * sizeof(complex double) );
	    }
	    fftw_execute_dft_r2c(shtns->fft,Vr, BrF);
	}
	imlim = MTR;
	#ifdef SHT_VAR_LTR
		if (imlim*MRES > (unsigned) llim) imlim = ((unsigned) llim)/MRES;		// 32bit mul and div should be faster
	#endif
  #endif

	ni = NLAT_2;	// copy NLAT_2 to a local variable for faster access (inner loop limit)
	//	im = 0;		// dzl.p = 0.0 : and evrything is REAL
  #ifndef SHT_NO_DCT
	#ifndef SHT_AXISYM
		#define BR0	((double *)reo)
		fftw_execute_r2r(shtns->dct_m0,(double *) BrF, BR0);		// DCT out-of-place.
	#else
		#define BR0	reo0
		fftw_execute_r2r(shtns->dct_m0,Vr, BR0);	// DCT out-of-place.
	#endif
		l=0;
		long int klim = shtns->klim;
		#ifdef SHT_VAR_LTR
			i = (llim * SHT_NL_ORDER) + 2;		// sum truncation
			if (i < klim) klim = i;
		#endif
		v2d* Ql = (v2d*) Qlm;
		zl = shtns->zlm_dct0;
		BR0[klim] = 0;		BR0[klim+1] = 0;		// allow some overflow.
	#ifdef SHT_VAR_LTR
		while(l < llim) {
	#else
		do {		// l < LMAX
	#endif
			i=l;	// l < klim
	  #ifndef _GCC_VEC_
			complex double q0 = 0.0;	complex double q1 = 0.0;
			do {
				q0 += BR0[i]   * zl[i];
				q1 += BR0[i+1] * zl[i+1];
				i+=2;
			} while(i<klim);
			Ql[l] = q0;		Ql[l+1] = q1;
	  #else
			s2d q = vdup(0.0);
			s2d q1 = vdup(0.0);
			i >>= 1;	// i = i/2
			do {
				q += ((s2d*) zl)[i] * ((s2d*) BR0)[i];
				++i;
				q1 += ((s2d*) zl)[i] * ((s2d*) BR0)[i];
				++i;
			} while(2*i < klim);
			q += q1;
			Ql[l]   = vlo_to_cplx(q);		Ql[l+1] = vhi_to_cplx(q);
	  #endif
			l+=2;
			zl += (shtns->klim - l);
	#ifndef SHT_VAR_LTR
		} while(l<llim);
	#else
		}
	#endif
		if (l == llim) {
			complex double q0 = 0.0;
			i=l;	// l < klim
			do {
				q0 += BR0[i] * zl[i];
				i+=2;
			} while(i<klim);
			((complex double *) Ql)[l] = q0;
			++l;
		}
	#undef BR0
	#ifdef SHT_VAR_LTR
		while( l<=LMAX ) {
			Ql[l] = vdup(0.0);
			++l;
		}
	#endif
  #else		// ifndef SHT_NO_DCT
		i=0;
		zl = shtns->zlm[0];
		// stride of source data : we assume NPHI>1 (else SHT_AXISYM should be defined).
	#ifndef SHT_AXISYM
		#define BR0(i) ((double*)BrF)[(i)*2]
	#else
		#define BR0(i) Vr[i]
	#endif
	#if _GCC_VEC_ && __SSE3__
		s2d r0v = vdup(0.0);
		do {	// compute symmetric and antisymmetric parts.
			s2d a = vdup(BR0(i));		s2d b = vdup(BR0(NLAT-1-i));
			s2d g = vdup(BR0(i+1));		s2d h = vdup(BR0(NLAT-2-i));
			a = subadd(a,b);
			g = subadd(g,h);
			((s2d*) reo0)[i] = a;		// assume odd is first, then even.
			((s2d*) reo0)[i+1] = g;		// assume odd is first, then even.
			a = _mm_unpackhi_pd(a, g);
			r0v += *((s2d*)(zl+i)) * a;	// even part is used, reduce data dependency
			i+=2;
		} while(i<ni);
		r0v += vxchg(r0v);
		((s2d*) reo0)[ni] = vdup(0.0);		// allow some overflow.
		((v2d*)Qlm)[0] = vhi_to_cplx(r0v);
	#else
		double r0 = 0.0;
		double r1 = 0.0;
		do {	// compute symmetric and antisymmetric parts.
			double a = BR0(i);		double b = BR0(NLAT-1-i);
			ro0(i) = (a-b);		re0(i) = (a+b);
			r0 += zl[i] * (a+b);
		} while(++i<ni);
		r0 += r1;
		ro0(ni) = 0.0;		re0(ni) = 0.0;		// allow some overflow.
		Qlm[0] = r0;
	#endif
		#undef BR0
		zl += ni + (ni&1);		// SSE alignement
		l=1;			// l=0 is zero for the vector transform.
		v2d* Ql = (v2d*) Qlm;		// virtual pointer for l=0 and im
	#ifdef SHT_VAR_LTR
		while (l<llim) {		// ops : NLAT/2 * (2*(LMAX-m+1) + 4) : almost twice as fast.
	#else
		do {
	#endif
			i=0;
  #ifndef _GCC_VEC_
			double q0 = 0.0;
			double q1 = 0.0;
			do {
				q0 += zl[0] * ro0(i);	// Qlm[LiM(l,im)] += zlm[im][(l-m)*NLAT/2 + i] * fp[i];
				q1 += zl[1] * re0(i);	// Qlm[LiM(l+1,im)] += zlm[im][(l+1-m)*NLAT/2 + i] * fm[i];
				zl +=2;
			} while(++i < ni);
			Ql[l] = q0;
			Ql[l+1] = q1;
  #else
			s2d q = vdup(0.0);
			s2d q1 = vdup(0.0);
			do {
				q += ((s2d*) zl)[i] * ((s2d*) reo0)[i];
				++i;
				q1 += ((s2d*) zl)[i] * ((s2d*) reo0)[i];		// reduce dependency
				++i;
			} while(i < ni);
			q += q1;
			zl += 2*ni;
			Ql[l] = vlo_to_cplx(q);		Ql[l+1] = vhi_to_cplx(q);
  #endif
			l+=2;
	#ifndef SHT_VAR_LTR
		} while (l<llim);
	#else
		}
	#endif
		if (l==llim) {
			long int lstride=1;
	  #ifdef SHT_VAR_LTR
			if (l != LMAX) lstride=2;
	  #endif
			double q1 = 0.0;		// reduce dependency by unrolling loop
			double q0 = 0.0;
			i=0;	do {
				q0 += zl[0] * ro0(i);		// Qlm[LiM(l,im)] += zlm[im][(l-m)*NLAT/2 + i] * fp[i];
				zl += lstride;
				++i;
				q1 += zl[0] * ro0(i);
				zl += lstride;
				++i;
			} while(i<ni);
			q0 += q1;
			((complex double *)Ql)[l] = q0;
	  #ifdef SHT_VAR_LTR
	  		++l;
		}
	    while( l<=LMAX ) {
			Ql[l] = vdup(0.0);
			++l;
      #endif
		}
  #endif		// ifndef SHT_NO_DCT
  #ifndef SHT_AXISYM
	for (im=1;im<=imlim;++im) {
		BrF += NLAT;
		i0 = shtns->tm[im];
 		i=i0;
		do {	// compute symmetric and antisymmetric parts.
			v2d q0 = ((v2d *)BrF)[i];	v2d q1 = ((v2d *)BrF)[NLAT-1-i];		re(i) = q0+q1;	ro(i) = q0-q1;		  
 		} while (++i<ni);
		l = LiM(shtns, 0,im);
		v2d* Ql = (v2d*) &Qlm[l];	// virtual pointer for l=0 and im
		l=im*MRES;
		zl = shtns->zlm[im];
		while (l<llim) {		// ops : NLAT/2 * (2*(LMAX-m+1) + 4) : almost twice as fast.
			v2d q0 = vdup(0.0);
			v2d q1 = vdup(0.0);
			i=i0;	do {		// tm[im] : polar optimization
				q0  += re(i) * ZL(0);		// Qlm[LiM(l,im)] += zlm[im][(l-m)*NLAT/2 + i] * fp[i];
				q1  += ro(i) * ZL(1);		// Qlm[LiM(l+1,im)] += zlm[im][(l+1-m)*NLAT/2 + i] * fm[i];
				zl +=2;
			} while (++i < ni);
			Ql[l] = q0;
			Ql[l+1] = q1;
			l+=2;
		}
		if (l==llim) {
			long int lstride=1;
	  #ifdef SHT_VAR_LTR
			if (l != LMAX) lstride=2;
	  #endif
			v2d q0 = vdup(0.0);	// Qlm[LiM(l,im)] = 0.0;
			i=i0;	do {		// tm[im] : polar optimization
				q0  += re(i) * ZL(0);		// Qlm[LiM(l,im)] += zlm[im][(l-m)*NLAT/2 + i] * fp[i];
				zl  += lstride;
			} while(++i<ni);
			Ql[l] = q0;
	  #ifdef SHT_VAR_LTR
	  		++l;
		}
	    while( l<=LMAX ) {
			Ql[l] = vdup(0.0);
			++l;
      #endif
		}
	}
	#ifdef SHT_VAR_LTR
	if (imlim < MMAX) {
		im = imlim+1;
		l = LiM(shtns, im*MRES, im);
		do {
			((v2d*)Qlm)[l] = vdup(0.0);
		} while(++l < shtns->nlm);
	}
	#endif

  	if (shtns->ncplx_fft > 0) {		// free memory
	    VFREE(BrF - NLAT*imlim);
	}
  #endif

	#undef ZL
	#undef re
	#undef ro
	#undef re0
	#undef ro0
	#undef reo0
	#undef reo0
  }
